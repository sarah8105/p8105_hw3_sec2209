---
title: "P8105 Homework 3"
author: "sarah_8105"
output: github_document
---

This is my third homework assignment for P8105.

```{r libraries}
library(tidyverse)
library(p8105.datasets)
```


## Problem 1

In this first problem, I explore the Instacart data. I create a plot of the number of items ordered in each aisle, limiting to aisles with more than 10,000 items ordered. 

```{r insta}
data("instacart")

```

Next, I create a table that displays the three most popular items in each of the aisles of "baking ingredients", "dog food care", and "packaged vegetables fruits". The table displays the number of times each item was ordered.

```{r insta_t1}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(orders = n()) %>%
  arrange(aisle, desc(orders)) %>%
  top_n(n = 3) %>%
  knitr::kable()
```

In the final portion of problem 1, I create a table that shows the mean hour of the day at which Pink Lady apples and coffee ice cream are ordered on each day of the week.

```{r insta_t2}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  arrange(product_name, order_dow) %>%
  pivot_wider(
    names_from = product_name,
    values_from = mean_hour
    ) %>%
  knitr::kable(digits = 1)
```

The Instacart data contain de-identified grocery orders for users of this online grocery service, which partners with local stores like Whole Foods and Fairway to deliver groceries within 2 hours of order. Each row in the data set represents a product that has been ordered. The data include variables about the order (such as date and time), the customer, and about the products ordered (such as product name and department/aisle where the product can be found). 

The resulting data set contains `r nrow(instacart)` rows  and `r ncol(instacart)` columns. There are a total of `r summarize(instacart, n_distinct(order_id))` orders, `r summarize(instacart, n_distinct(user_id))` customers, and `r summarize(instacart, n_distinct(product_id))` products represented in these data. On average, customers have placed `r instacart %>% group_by(user_id) %>% summarize(orders = n_distinct(order_id)) %>% summarize(mean(orders)) %>% round()` orders and each order contains `r instacart %>% group_by(order_id) %>% summarize(products = n()) %>% summarize(mean(products)) %>% round()` products. 

There are `r summarize(instacart, n_distinct(aisle_id))` aisles represented in the data. Most items that are ordered come from the `r instacart %>% group_by(aisle) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(aisle)` aisle, with `r instacart %>% group_by(aisle) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(items)` items ordered. The most popular item from the baking ingredients aisle was `r instacart %>% filter(aisle == "baking ingredients") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`, the most population item from the dog food & care aisle was `r instacart %>% filter(aisle == "dog food care") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`, and the most population item from the packaged vegetables & fruits aisle was `r instacart %>% filter(aisle == "packaged vegetables fruits") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`. Coffee ice cream tended to be ordered later in the day than pink lady apples.

 
## Problem 2

In this code chunk, I load and tidy the accelerometer data. I use the `clean_names()` function from the `janitor` package to clean the variable names, create a weekday versus weekend variable, and encode data with reasonable variable classes. 


```{r accel}
accel = 
  read_csv(
    "./Data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_count",
    values_to = "active",
    names_prefix = "activity_"
  ) %>%
  mutate(weekend = ifelse(day_id %in% c(3, 4), "Weekend", "Weekday"))
```

These data represent five weeks of accelerometer data collected on a 63 year-old Advanced Cardiac Care Center of Columbia University Medical Center patient who has been diagnosed with the chronic conditions of congestive heart failure and obesity. The resulting data set contains `r nrow(accel)` rows and `r ncol(accel)` variables. This data set contains indicator variables for the week number (`week`) and day of the week (`day`), a variable `activity_count` that represents incremental one-minute measurement interval of the accelerometer, a variable that represents the accelerometer reading (`active`)  and the `weekend` indicator that defines whether it was a weekend or weekday.

In the next code chunk, I aggregate the activity totals for each day and display the data in a table.


```{r accel_day}
accel %>%
  group_by(week, day) %>%
  summarize(
    total_active = sum(active, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = week,
    values_from = total_active,
    names_prefix = "Week "
  ) %>%
  knitr::kable(digits = 1)
```

From this table, there appears to be a bit of a decline in activity on the weekends.

In the next code chunk, I create a single-panel plot that displays the 24-hour activity trends for each day, color-coded by day of the week.

```{r accel_24}
ggplot(accel, aes(x = activity_count, y = active)) + 
  geom_point(aes(color = day), alpha = .5) +
  geom_smooth(se = FALSE)
```



## Problem 3

In this final problem, I explore and clean the NY NOAA data. First, I separate year, month, and day. I then change the units for temperature from tenths of degrees C to degrees C and precipitation from tenths of mm to mm.

```{r noaa}
data("ny_noaa")

ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = as.integer(month),
         day = as.integer(day),
         year = as.integer(year),
         prcp = prcp / 10,
         tmax = as.integer(tmax) / 10,
         tmin = as.integer(tmin) / 10
         ) 
```

The National Oceanic and Atmospheric Association (NOAA) National Climatic Data Center data contain summary statistics from weather stations across the country. This subset of the NOAA data contain daily summary statistics on precipitation amount (`prcp`), snowfall amount (`snow`), snow depth (`snwd`), and minimum and maximum temperature (`tmax` and `tmin`) from the New York state weather stations from 1981-2010. 

The resulting data set contains `r nrow(ny_noaa)` rows  and `r ncol(ny_noaa)` columns. There is substantial missing data, with `r ny_noaa %>% summarize(mean(is.na(tmax))) * 100 %>% round()`% of observations missing maximum temperature, `r ny_noaa %>% summarize(mean(is.na(tmin))) * 100 %>% round()`% of observations missing minimum temperature, `r ny_noaa %>% summarize(mean(is.na(snow))) * 100 %>% round()`% of observations missing snow amount, and `r ny_noaa %>% summarize(mean(is.na(snwd))) * 100 %>% round()`% of observations missing snow depth. Since it rarely snows in New York outside of winter months, the most common observed snowfall values is`r summarize(ny_noaa, mode(snwd))` orders,

In the next code chunk, I create a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r noaa_plot1}
ggplot(ny_noaa, aes(x = id, y = tmax)) + geom_boxplot()
```

