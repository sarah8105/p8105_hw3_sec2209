---
title: "P8105 Homework 3"
author: "sarah_8105"
output: github_document
---

This is my third homework assignment for P8105.

```{r libraries}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

In this first problem, I explore the Instacart data. I create a plot of the number of items ordered in each aisle, limiting to aisles with more than 10,000 items ordered. 

```{r insta_p1}
data("instacart")

instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
  
```

Next, I create a table that displays the three most popular items in each of the aisles of "baking ingredients", "dog food care", and "packaged vegetables fruits". The table displays the number of times each item was ordered.

```{r insta_t1}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

In the final portion of problem 1, I create a table that shows the mean hour of the day at which Pink Lady apples and coffee ice cream are ordered on each day of the week.

```{r insta_t2}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  arrange(product_name, order_dow) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
    ) %>%
  knitr::kable(digits = 1)
```

The Instacart data contain de-identified grocery orders for users of this online grocery service, which partners with local stores like Whole Foods and Fairway to deliver groceries within 2 hours of order. Each row in the data set represents a product that has been ordered. The data include variables about the order (such as date and time), the customer, and about the products ordered (such as product name and department/aisle where the product can be found). 

The resulting data set contains `r nrow(instacart)` rows  and `r ncol(instacart)` columns. There are a total of `r summarize(instacart, n_distinct(order_id))` orders, `r summarize(instacart, n_distinct(user_id))` customers, and `r summarize(instacart, n_distinct(product_id))` products represented in these data. On average, customers have placed `r instacart %>% group_by(user_id) %>% summarize(orders = n_distinct(order_id)) %>% summarize(mean(orders)) %>% round()` orders and each order contains `r instacart %>% group_by(order_id) %>% summarize(products = n()) %>% summarize(mean(products)) %>% round()` products. 

There are `r summarize(instacart, n_distinct(aisle_id))` aisles represented in the data. Most items that are ordered come from the `r instacart %>% group_by(aisle) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(aisle)` aisle, with `r instacart %>% group_by(aisle) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(items)` items ordered. The most popular item from the baking ingredients aisle was `r instacart %>% filter(aisle == "baking ingredients") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`, the most population item from the dog food & care aisle was `r instacart %>% filter(aisle == "dog food care") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`, and the most population item from the packaged vegetables & fruits aisle was `r instacart %>% filter(aisle == "packaged vegetables fruits") %>% group_by(product_name) %>% summarize(items = n()) %>% arrange(desc(items)) %>% top_n(n = 1) %>% pull(product_name)`. Coffee ice cream tended to be ordered later in the day than pink lady apples.

 
## Problem 2

In this code chunk, I load and tidy the accelerometer data. I use the `clean_names()` function from the `janitor` package to clean the variable names, create a weekday versus weekend variable, and convert variable types. 


```{r accel}
accel = 
  read_csv(
    "./Data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_count",
    values_to = "active",
    names_prefix = "activity_"
  ) %>%
  mutate(
    weekend = as.factor(ifelse(day %in% c("Saturday", "Sunday"), "Weekend", "Weekday")),
    day = as.factor(day),
    day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")),
    activity_count = as.numeric(activity_count),
    day_id = as.factor(day_id)
    )
```

These data represent five weeks of accelerometer data collected on a 63 year-old Advanced Cardiac Care Center of Columbia University Medical Center patient who has been diagnosed with the chronic conditions of congestive heart failure and obesity. The resulting data set contains `r nrow(accel)` rows and `r ncol(accel)` variables. This data set contains indicator variables for the week number (`week`) and day of the week (`day`), a variable (`activity_count`) that represents incremental one-minute measurement intervals of the accelerometer, a variable that represents the accelerometer reading (`active`)  and the `weekend` indicator that defines whether it was a weekend or weekday.

In the next code chunk, I aggregate the activity totals for each day and display the data in a table.


```{r accel_day}
accel %>%
  group_by(week, day) %>%
  summarize(
    total_active = sum(active, na.rm = TRUE)) %>% 
  pivot_wider(
    names_from = week,
    values_from = total_active,
    names_prefix = "Week "
  ) %>%
  knitr::kable(digits = 1)
```

From this table, there appears as though the patient was not active or was not using the accelerometer on Saturdays in weeks 4 and 5. On these days, the patient had an activity count of 1,440, which is likely a default activity count of 1 at each one-minute measurement interval. The patient also had lower activity levels on Sunday of weeks 4 and 5 than in the first 3 weeks of observation.

In the next code chunk, I create a single-panel plot that displays the 24-hour activity trends for each day, displaying average trends color-coded by day of the week.  

```{r accel_24}
accel %>%
  group_by(day_id) %>%
  ggplot(aes(x = activity_count, y = active, color = day)) + 
  geom_line(color = "black", alpha = .2) +
  stat_smooth() +
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440),
    labels = c("12 AM", "6 AM", "12 PM", "6 PM", "11:59 PM"),
  ) +
  labs(
    title = "Patterns in 24 Hour Activity Time Courses",
    x = "Hour of Day",
    y = "Accelerometer Activity Count",
    color = "Day of Week"
  )
```

Based on this graph, this patient begins increasing their activity level around 6 AM. The patient has some moments of high activity, particularly in the evening. On average, activity level does not differ by day of week, with some minor exceptions. For example, on Sundays there is an increase in activity around 11 AM - 12 PM. During Fridays, there is an increase in activity around 8-9 PM.

## Problem 3

In this final problem, I explore and clean the NY NOAA data. First, I separate year, month, and day. I then change the units for temperature from tenths of degrees C to degrees C and precipitation from tenths of mm to mm. I then count the instance of the different values of snowfall to determine the most common snowfall amount.

```{r noaa}
data("ny_noaa")

month_df =
  tibble(
    month = 1:12,
    month_name = month.name,
    month_abbr = month.abb
  )

noaa = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = as.integer(month),
         day = as.integer(day),
         year = as.integer(year),
         prcp = prcp / 10,
         tmax = as.integer(tmax) / 10,
         tmin = as.integer(tmin) / 10,
         ) %>%
  left_join(month_df, by = "month") %>%
  select(-month) %>%
  rename(month = month_name) %>%
  mutate(month = forcats::fct_relevel(month, c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November")))

noaa %>%
  count(snow) %>%
  arrange(desc(n)) %>%
  top_n(1)
```

The National Oceanic and Atmospheric Association (NOAA) National Climatic Data Center data contain summary statistics from weather stations across the country. This subset of the NOAA data contain daily summary statistics on precipitation amount (`prcp`), snowfall amount (`snow`), snow depth (`snwd`), and minimum and maximum temperature (`tmax` and `tmin`) from the New York state weather stations from 1981-2010. 

The resulting data set contains `r nrow(ny_noaa)` rows  and `r ncol(ny_noaa)` columns. There is substantial missing data, with `r ny_noaa %>% summarize(mean(is.na(tmax))) %>% round(2) * 100`% of observations missing maximum temperature, `r ny_noaa %>% summarize(mean(is.na(tmin))) %>% round(2) * 100`% of observations missing minimum temperature, `r ny_noaa %>% summarize(mean(is.na(snow))) %>% round(2) * 100`% of observations missing snow amount, and `r ny_noaa %>% summarize(mean(is.na(snwd))) %>% round(2) * 100 `% of observations missing snow depth. Since it rarely snows in New York outside of winter months, the most common observed snowfall values is`r summarize(ny_noaa, mode(snwd))` mm.

In the next code chunk, I create a two-panel plot showing the average max temperature in January and in July in each station across years. 

```{r noaa_plot1}
noaa %>%
  select(id, month, year, tmax) %>%
  filter(month %in% c("January", "July")) %>%
  group_by(id, month, year) %>%
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>%
  drop_na(mean_tmax) %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = month)) + 
  geom_line(alpha = .5) +
  facet_grid(. ~ month) +
  labs(
    title = "Distribution in the Average Maximum Temperature at the NY NOAA Stations across 1981-2010",
    x = "Year",
    y = "Average Maximum Temperature (Degrees Celsius)",
    color = "Month"
  )
```

From this graph, we can see that average maximum temperatures in July are much higher than temperatures in January, as expected. There is also a wider range in the January temperatures across NY NOAA stations than in the July temperatures. There are a few noticeable outliers, with one station having a temperature below 15 degrees Celsius in July and another station having a temperaure below -5 degrees Celsius in January.

In this next code chunk, I create a two-panel plot showing `tmax` and `tmin` for the full data set and showing the distribution of snowfall values great than 0 and less than 100 separately by year.

```{r noaa_plot2}
plot_temp = noaa %>%
  select(id, tmax, tmin) %>%
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex(alpha = .5) +
  scale_x_continuous(
    breaks = c(-60, -30, 0, 30, 60),
    labels = c("-60 C", "-30 C", "0 C", "30 C", "60 C"),
  ) +
  scale_y_continuous(
    breaks = c(-60, -30, 0, 30, 60),
    labels = c("-60 C", "-30 C", "0 C", "30 C", "60 C"),
  ) +
  labs(
     title = "Plot of Minimum and Maximum Temperatures",
    x = "Maximum Temperature (Degrees Celsius)",
     y = "Minimum Temperature (Degrees Celsius)"
    ) +
  theme(legend.position = "right")

plot_snow = noaa %>%
  filter(snow > 0, snow < 100) %>%
  mutate(year = as.factor(year)) %>%
  drop_na(snow) %>%
  ggplot(aes(x = year, y = snow)) + 
  geom_violin(color = "blue") +
  stat_summary() +
  scale_x_discrete(
      breaks = c(1981, 1990, 2000, 2010)
      ) +
    labs(
      title = "Distribution of Snowfall by Year",
      x = "Year",
      y = "Snowfall",
      caption = "Including snowfall greater than 0 and less than 100mm and mean snowfall in black."
    ) 

plot_temp / plot_snow 
```

From this graph, we can see that the distribution in minimum and maximum temperatures are offset by approximately 10-15 degrees Celsius. The distribution in snowfall after removing days with 0 mm or greater than 100 mm of snow is relatively consistent across the years, with an average around 30 mm and a right-skewed distribution or long tail towards upper snowfall amounts.

